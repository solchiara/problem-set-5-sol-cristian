---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Sol Rivas Lopes - 12218930
    - Partner 2 (name and cnet ID): Cristian Bancayan - 
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: **SCRL** \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
from bs4 import BeautifulSoup
import requests
from datetime import datetime
import os


import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}

# Creating empty vector for data
all_enforcement_data = []

# Looping through each page
for page_num in range(1, 481):  # 480 pages total
    # Construct the URL for each page
    url = f"https://oig.hhs.gov/fraud/enforcement/?page={page_num}"
    response = requests.get(url)
    
    # Check if the request was successful
    if response.status_code != 200:
        print(f"Failed to retrieve page {page_num}")
        continue
    
    # Parse the page content
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Extract each enforcement action on the page
    for action in soup.find_all("li", class_="usa-card card--list pep-card--minimal mobile:grid-col-12"):
        # Extract title
        title_tag = action.find("h2", class_="usa-card__heading")
        title = title_tag.get_text(strip=True)
        
        # Extract link
        link = title_tag.find("a")["href"]
        full_link = f"https://oig.hhs.gov{link}"  # Ensure it's a full URL
        
        # Extract date
        date = action.find("span", class_="text-base-dark padding-right-105").get_text(strip=True)
        
        # Extract category
        category = action.find("li", class_="display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1").get_text(strip=True)
        
        # Append to list as dictionary
        all_enforcement_data.append({
            "Title": title,
            "Date": date,
            "Category": category,
            "Link": full_link
        })
    
    # Track progress
    print(f"Completed page {page_num}")
    
    # Delay to avoid being blocked
    time.sleep(1)

# Convert the list to a DataFrame
df_enforcement = pd.DataFrame(all_enforcement_data)

# Writing a csv to store this locally so I don't have to run 35min code every single time


df_enforcement.to_csv("enforcement_actions.csv", index=False)

```

  
### 2. Crawling (PARTNER 1)

```{python}

# Since data for this problem set is only needed starting from January 2021
# I am dropping older entries from this df
# Crawler already took 90min for this restricted database
#  I will not run this over 7000x more to retrieve info that will not be necessary. 
df_enforcement = df_enforcement[0:2995]


# putting links in a list
links = df_enforcement["Link"]

# Empty list 
agency_info = []


# Loop through each enforcement action link in the DataFrame to retrieve the agency info
for index in range(0, 2995):
    url = links[index]
    response = requests.get(url)
    
    # Check if the request was successful
    if response.status_code != 200:
        print(f"Failed to retrieve agency info for {url}")
        agency_info.append(None)  # Append None if there's an error
        continue
    
    # Parse the page content
    soup = BeautifulSoup(response.text, "html.parser")
    
    # Find the <li> element, where <span> is
    agency_tag = None
    for li in soup.find_all("li"):
      #find <span> subelement where agency information is
        if li.find("span", class_="padding-right-2 text-base") and "Agency:" in li.get_text():
            agency_tag = li
            break  # Stop once we find the first matching <li> with "Agency:"
    
    # Extract the agency information if found
    if agency_tag:
        agency = agency_tag.get_text(strip=True).replace("Agency:", "").strip()
    # If it doesn't find any info on agency, say non
    else:
        agency = None
    
    # Append the agency information to object
    agency_info.append(agency)
    
    # Track progress
    print(f"Completed retrieving agency info for row {index}")
    
    # Delay to avoid being blocked
    time.sleep(1)

# Add the agency information to the DataFrame as a new column
df_enforcement["Agency"] = agency_info

print(df_enforcement)

# Writing a csv to store this locally
df_enforcement.to_csv("enforcement_actions_with_agency.csv", index=False)


```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}

```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

base_path = r"C:\Users\solch\OneDrive\Documentos\2024 - autumn quarter\python II\problem-set-5-sol-cristian"

os.chdir(base_path)

path = os.path.join(
    base_path, "enforcement_actions_with_agency.csv")


df_enforcement = pd.read_csv(path)

# Convert to datetime object
df_enforcement["Date"] = pd.to_datetime(df_enforcement["Date"], format="%B %d, %Y")

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

df_enforcement_subset = df_enforcement[
    (df_enforcement["Category"] == "Criminal and Civil Actions") | 
    (df_enforcement["Category"] == "State Enforcement Agencies")
]

df_enforcement["Period"] = df_enforcement["Date"].dt.to_period("M")


df_enforcement_subset = df_enforcement_subset.groupby("Category", "Period").size()

alt.Chart(df_enforcement_subset, title="Number of Enforcement Actions by Month"
    ).mark_line().encode(
    alt.X("Period:T", title="Period"),
    alt.Y("PRVDR_NUM:Q", title="Number of Unique Observations")
).configure_axis(
    labelFontSize=8,
    titleFontSize=12
)



```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```